{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "About The Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gn5e8RD6me"
      },
      "source": [
        "# (1) Introduction\n",
        "This project was created for the sole purpose of trying to decode the ideal price of a phone given a specific set of features it has. This algorithm has various uses, for example, it can be used for determining the price category of an upcoming smartphone, based on its leaked features, or can help a smartphone startup determine what price the business must price their smartphones for or heck even try to find out if you, as the user had been scammed for the phone you currently own right now!!! This is a relatively simple algorithm focused more on the explanation side as I start my journey to **broadcast** all my projects for a free download for any user to use. Anyways lets begin!!!\n",
        "\n",
        "Let's begin with the characteristics this algorithm sees fit. Mentioned below are the parameters (features) used to train this model.\n",
        "\n",
        "Btw here is the **[dataset](https://www.kaggle.com/iabhishekofficial/mobile-price-classification)**.\n",
        "\n",
        "\n",
        "## Independent Variables\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Battery power (mAh).\n",
        "2. Has bluetooth?\n",
        "3. Microprocessor clock_speed (GHZ).\n",
        "4. Has dual sim??\n",
        "5. Mega Pixels of selfie camera.\n",
        "6. Has 4G?\n",
        "7. Memory (Gb).\n",
        "8. Depth (cm).\n",
        "9. Weight (g).\n",
        "10. Number of cores.\n",
        "11. Primary camera Mega Pixels.\n",
        "12. Pixel Resolution Height (pix).\n",
        "13. Pixel Resolution Width (pix).\n",
        "14. RAM (Mb).\n",
        "15. Height of screen (cm).\n",
        "16. Width of screen (cm).\n",
        "17. Longest time that a single battery charge will last (hrs).\n",
        "18. Has 3G?\n",
        "19. Is touck screen?\n",
        "20. Has wifi?\n",
        "---\n",
        "Here is what we will be predicting.\n",
        "\n",
        "## Dependent Vairable\n",
        "1. Price range.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaTIUdHrFzNJ"
      },
      "source": [
        "# (2) Importing the necessary libraries\n",
        "Below is the description of libraries and their use\n",
        "\n",
        "| Library      | Use |\n",
        "| ----------- | ----------- |\n",
        "| Numpy      | Used to deal with arrays (i.e advanced lists)      |\n",
        "| Pandas   | Used to get the data from the CSV file \"test.csv\" & \"train.csv\"        |\n",
        "| Matplotlib      | Used to plot graphs to add a visual sense to our algorithm      |\n",
        "| Sklearn   | This is a massive library with a lot of sub-functions but the one we imported was the **StandardScaler** module which basically used for formatting data into a more consistent form. We will revisit this later down the road |\n",
        "| Keras.models.Sequential      |    This library is used to initialise a framework into which we can add/customize our own neural network   |\n",
        "| Keras.layers.Dense     |    This library is used to initialize a new basic node into a neural network   |\n",
        "| Keras.layers.Dropout     |    This library is used to add dropout (will be explained later)  |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adQZ6-akGPm5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3_JCLzQIt2G"
      },
      "source": [
        "# (3) Data preprocessing\n",
        "This is a crucial step for any machine learning algorithm as every library or algorithm has its own prerequisites if you will, that is, each algorithm has a different set of format. Just look out for them before you build any model. The b/m code cell deals with the extraction and the reformatting of data. The hash-tag after each line explains what each individual line of code does. An example is provided below.\n",
        "\n",
        "```\n",
        "code # Explanation provided\n",
        "```\n",
        "Lets see what the b/m code cell means\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGkckn_yKO16"
      },
      "source": [
        "## (3a) Extracting the data & formatting it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkVwC3SJJmXL"
      },
      "source": [
        "dataset = pd.read_csv(\"train.csv\") # Getting the data from the \"train.csv excel sheet\"\n",
        "features = dataset.iloc[:, :-1].values # Extracting every row but the last one & converting it to an array (Getting the features)\n",
        "price_range = dataset.iloc[:, -1].values # Extracting the last last column & converting it to an array (Getting the price range)\n",
        "price_range = np.array(price_range).reshape(-1, 1) # Formatting the data from a 1d array to a 2d one for it to be fitted into the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3WlbIr4LF0X"
      },
      "source": [
        "## (3b) Applying a Standard Scaler\n",
        "The [Standard Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) function is applied to the ***training set***, which is referred as features in this algorithm. Its purpose is to scale all the data into a more uniform \"range\" i.e many datasets will have one parameter having a range from (1000 - 10000) and another one from (0.01 - 0.1). This is dangerous while training as when some ***mathematical functions*** are applied the inconsistency can ruin the balance of the [weights](https://) which we will get into when I explain the model to you in upcomming sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGXCzU2GMmGm"
      },
      "source": [
        "scaler = StandardScaler() # Initializing the scaler object\n",
        "features = scaler.fit_transform(features) # Fitting the scaler into the training_set (A.K.A  features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ODnALWNDYs"
      },
      "source": [
        "# (4) The Artificial Neural Network (ANN)"
      ]
    }
  ]
}